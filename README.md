![image](https://github.com/user-attachments/assets/ea249019-44b2-4cb7-9027-e45729667946)

# Argument 

Extracting semantically meaningful visual representations from unlabeled images and videos is crucial for developing vision foundation models. Given that images and videos represent the most extensive visual signals at the internet scale, they are pivotal for training such models.  Recent methodologies have made strides by extracting contextual information from images and temporal patterns from video sequences. We posit that the incorporation of intrinsic 3D geometric insights can substantially elevate a model's interpretation of the visual world. Pursuing this angle, we design a framework that infers depth, motion, and contextual image information in a self-supervised paradigm through a unified encoder architecture, leveraging solely unlabeled 2D image data. Thus, learned content features will incorporate kinetic and structural information. We substantiate our theory by achieving on-par performance with existing methods in unsupervised optical flow and depth learning benchmarks and rigorously evaluating the extracted features across a spectrum of downstream tasks, demonstrating the tangible benefits of 3D geometric awareness in enriching visual understanding.

# World models 
The learning capabilities of humans, animals, and current AI and ML systems exhibit a significant difference While humans and animals quickly grasp complex tasks with minimal exposure, ML systems require extensive data and struggle to match human-like reliability. Human and animal learning efficiency stems from their ability to develop internal world models, enabling prediction, reasoning, and adaptation with limited interactions. AI research tackles challenges like learning from observation and implementing reasoning compatible with gradient-based learning. Proposed solutions include cognitive architectures with differentiable modules and predictive world models. Notable contributions involve a non-contrastive self-supervised learning paradigm and leveraging hierarchical predictive world models for planning under uncertainty. The excerpt underscores the necessity of crafting learning paradigms and architectures for machines to autonomously acquire and apply world models.
In world models paper (2018) delves into the human brain's predictive model and its relevance to reinforcement learning (RL). It introduces a method to efficiently train large neural network agents by dividing them into world and controller models. By leveraging the world model, agents can learn compact policies for RL tasks. The discussion encompasses key concepts from related papers and offers insights for applying these ideas in RL environments.

# JEPA architecture

![image](https://github.com/user-attachments/assets/816f60ec-2e30-44a1-958f-02c845d1912d)

The JEPA (Jointly Embeding Predictive Architecture ) architecture is designed for learning by predicting one part of an input (context) using another part of the input (target) within an abstract latent space. First work of this method is In I-JEPA  (2023) (Joint-embedding Predictive Architecture for Images) they propose a novel approach aimed to improve the semantic level of self-supervised representations without relying on extra prior knowledge encoded through image transformations. It achieves this by predicting missing information in an abstract representation space. The method predicts representations of various target blocks in an image given a single context block, thereby encouraging the learning of more semantic features. Next work is MC-JEPA(2023), a novel framework merging optical flow estimation and self-supervised learning within a shared encoder. Traditionally, self-supervised learning focuses solely on content features, while optical flow estimation solely on motion. MC-JEPA combines both, showing mutual benefit, yielding content features with motion information. It achieves performance comparable to existing benchmarks in unsupervised optical flow and self-supervised learning, notably enhancing downstream tasks like semantic segmentation. In V-JEPA(2024), extending the JEPA learning principle to video data by training a visual encoder to predict missing regions in a spatio-temporally masked video. This approach yields strong off-the-shelf performance on various image and video tasks. V-JEPA achieves significant boosts in semantic temporal understanding tasks and image-level feature tasks, obtaining the highest unsupervised video model accuracy on Kinetics and ImageNet.

# SSL contrastive content learning
VICReg (Variance-Invariance-Covariance Regularization) is introduced to address the collapse problem in contrastive content learning methods for image representation learning. It prevents encoder-produced embeddings from becoming constant or non-informative by maintaining variance and decorrelating variables in each embedding dimension. Unlike other approaches, VICReg doesn't rely on weight sharing or normalization techniques and achieves state-of-the-art results on various downstream tasks. 
Downstream image recognition tasks, including linear head and semi-supervised evaluation protocols for image classification on datasets like ImageNet.Additionally, VICReg stabilises training for other methods and enhances their performance through its variance regularisation term. VICReg does not require a memory bank, contrastive samples, large batch sizes, batch-wise or feature-wise normalization, vector quantization, or a predictor module. This simplicity sets it apart from other methods that may require more complex operations. VICReg offers a straightforward yet powerful solution to the collapse problem in self-supervised contrastive learning.

#Flow estimation
![image](https://github.com/user-attachments/assets/8ab50e1e-bf22-42d4-9791-5889789bc1e0)

The PWC-Net (2018) addresses the challenge of optimizing optical flow estimation models for both accuracy and size. By combining classical principles with deep learning, PWC-Net achieves significant improvements over existing CNN models. PWC-Net utilizes pyramid, warping, and cost volume principles in optical flow estimation as follows:
1. Feature Pyramid Extraction: Given two input images $ I1 and  I2, the Siamese network encodes these images to generate L-level pyramids of feature representations. The bottom (zeroth) level of the pyramid corresponds to the input images. Each level of the pyramid is created by downsampling the features from the previous level using convolutional filters. The number of feature channels increases as we move up the pyramid levels.
2. Optical Flow Estimation: At each pyramid level, an optical flow estimator network processes the features to estimate the optical flow. This network consists of convolutional layers followed by activation functions like leaky ReLU, with the final layer outputting the optical flow. The network learns to predict the optical flow based on the features extracted from the images.
3. Warping and Cost Volume Construction: The current optical flow estimate is used to warp the CNN features of the second image. These warped features, along with the features of the first image, are used to construct a cost volume. The cost volume captures the discrepancy between the features of the two images after warping, providing a more discriminative representation for optical flow estimation.
4. CNN Processing: The cost volume is processed by additional CNN layers to refine and predict the final optical flow map. The CNN layers learn to extract relevant information from the cost volume and generate the optical flow predictions. The network architecture and training process enable the model to learn the complex relationships between image features and optical flow.
5. Context Network: To further refine the optical flow predictions, a context network is used to incorporate contextual information and improve the accuracy of the final optical flow map. This network leverages additional features to enhance the optical flow estimation results.
By following these steps, starting from feature pyramid extraction to the prediction of the final optical flow map, PWC-Net effectively utilizes pyramid, warping, and cost volume principles to estimate optical flow accurately and efficiently.
This has been the baseline paper in many of the optical flow estimation models which followed.

#Depth estimation


Monodepth2 (2018) introduces enhancements to self-supervised learning for monocular depth estimation, addressing the challenge of acquiring per-pixel ground-truth depth data at scale. Despite the trend towards increasingly complex architectures, the proposed approach relies on a surprisingly simple model and design choices.
The depth prediction network takes a single color image as input and produces a depth map. The training is framed as a novel view-synthesis problem, where the network is trained to predict the appearance of a target image from the viewpoint of another image. By constraining the network to perform image synthesis using an intermediary variable (depth or disparity), interpretable depth information is extracted from the model. The network is trained to predict depth maps that can correctly reconstruct novel views given the relative pose between two images. This is an ill-posed problem due to the large number of possible incorrect depths per pixel. Traditional stereo methods address this ambiguity by enforcing smoothness in depth maps and computing photo-consistency on patches during global optimization. [Monodepth2] introduces several contributions to improve depth estimation results, including a minimum reprojection loss to handle occlusions robustly, a full-resolution multi-scale sampling method to reduce visual artifacts, an auto-masking loss to ignore training pixels that violate camera motion assumptions.
   But [monodepth2] struggle to predict depth for moving objects, such as cars. While the Monodepth2 method succeeds in handling moving objects better than some other methods, there are still challenges in accurately estimating depth for dynamic scenes.

#Joint flow-depth estimation
TrianFlow (2021) addresses scale inconsistency in self-supervised joint depth-pose learning by disentangling scale from network estimation. It proposes a system that recovers relative pose using dense optical flow and triangulation aligns depth prediction scale with the triangulated point cloud, and achieves state-of-the-art performance on various datasets.
Self-mono-sf (2020) introduces Mono-SF, a novel monocular 3D scene flow estimation method that combines multi-view geometry and single-view depth information. It incorporates a convolutional neural network called ProbDepthNet, which estimates pixel-wise depth distributions, leading to improved performance compared to existing monocular baselines.
Multi-mono-sf(2019)  presents a novel monocular scene flow method using a single CNN to estimate depth and 3D motion from optical flow cost volumes. Leveraging self-supervised learning with 3D loss functions and occlusion reasoning, the model achieves state-of-the-art accuracy among unsupervised approaches, with competitive results for optical flow and monocular depth estimation.

#Spatio-Temporal feature modeling
In the realm of spatio-temporal feature modeling for video understanding, two prominent approaches have emerged in recent literature. The first, as proposed by Bertasius et al. (
?), introduces TimeSformer, a novel convolution-free method that leverages self-attention mechanisms over space and time. By adapting the Transformer architecture to video data and employing 'divided attention,' which separately applies temporal and spatial attention within each block, TimeSformer achieves state-of-the-art results in video classification tasks, including surpassing the best-reported accuracy on Kinetics-400 and Kinetics-600 datasets. This approach not only outperforms 3D convolutional networks in accuracy but also offers faster training, higher test efficiency, and the ability to process longer video clips. The second approach, as presented by Messina et al. (Recurrent Vision Transformer for Solving Visual Reasoning Problems), introduces the Recurrent Vision Transformer (RViT) model. By incorporating recurrent connections and spatial attention, RViT demonstrates competitive results in visual reasoning tasks, particularly on the SVRT dataset's same-different visual reasoning problems. The model's hybrid architecture, combining CNNs with Transformers, along with weight-sharing in spatial and depth dimensions, contributes to its regularization and ability to learn effectively with fewer parameters. The study also underscores the importance of feedback connections in iteratively refining internal representations for stable predictions, opening avenues for deeper exploration of attention and recurrent connections in visual abstract reasoning tasks.




