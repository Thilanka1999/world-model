{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_deep_loss_pack(loss_pack, key=\"\"):\n",
                "    if type(loss_pack)!=dict:\n",
                "        print(f\"{key}: {loss_pack.item() if loss_pack is not None else None}\")\n",
                "    else:\n",
                "        for k, v in loss_pack.items():\n",
                "            print_deep_loss_pack(v, f\"{key}:{k}\" if key !=\"\" else k)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ContentLoss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.learners import ContentLearner\n",
                "from src.models import BackBone\n",
                "from src.datasets import ImageNetVICReg\n",
                "from src.losses import ContentLoss\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "ds = ImageNetVICReg(\"./data/ImageNet-2012\")\n",
                "dl = DataLoader(ds, 8)\n",
                "\n",
                "bb = BackBone(\"ConvNeXt\").cuda(0)\n",
                "ln = ContentLearner(bb)\n",
                "ln.set_devices([0,1])\n",
                "loss_fn = ContentLoss()\n",
                "\n",
                "for batch in dl:\n",
                "    info = ln(batch)\n",
                "    loss_pack = loss_fn(info, batch)\n",
                "    print_deep_loss_pack(loss_pack)\n",
                "    break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.losses import ContentLoss\n",
                "import torch\n",
                "from mt_pipe.src.test.external.util import validate_nested_obj\n",
                "\n",
                "loss_fn = ContentLoss()\n",
                "mock_info = {\n",
                "    \"X_one\": torch.Tensor(8, 768),\n",
                "    \"X_two\": torch.Tensor(8, 768),\n",
                "    \"Y_one\": torch.Tensor(8, 8192),\n",
                "    \"Y_two\": torch.Tensor(8, 8192),\n",
                "}\n",
                "mock_batch = {\n",
                "    \"view1\": torch.Tensor(8, 3, 128, 128),\n",
                "    \"view2\": torch.Tensor(8, 3, 128, 128),\n",
                "}\n",
                "loss_pack = loss_fn(mock_info, mock_batch)\n",
                "expected_loss_pack_conf = {\n",
                "    \"tot\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "    \"Content_X\": {\n",
                "        \"tot\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "        \"Inv\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "        \"Var\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "        \"Cov\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "    },\n",
                "    \"Content_Y\": {\n",
                "        \"tot\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "        \"Inv\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "        \"Var\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "        \"Cov\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "    },\n",
                "}\n",
                "valid, msg = validate_nested_obj(loss_pack, expected_loss_pack_conf)\n",
                "assert valid, msg"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# FlowLoss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## SSL"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.learners import FlowLearner\n",
                "from src.models import BackBone\n",
                "from src.datasets import KITTI\n",
                "from src.losses import SSLFlowLoss\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "ds = KITTI(\"./data/KITTI-2012\", img_wh=[128,64])\n",
                "dl = DataLoader(ds, 8)\n",
                "\n",
                "bb = BackBone(\"ConvNeXt\").cuda(0)\n",
                "ln = FlowLearner(bb)\n",
                "ln.set_devices([0, 1])\n",
                "\n",
                "loss_fn = SSLFlowLoss(1, loss_weights={\"loss_pixel\": 9})\n",
                "\n",
                "for batch in dl:\n",
                "    info = ln(batch)\n",
                "    loss_pack = loss_fn(info, batch)\n",
                "    print_deep_loss_pack(loss_pack)\n",
                "    break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.losses import SSLFlowLoss\n",
                "import torch\n",
                "from mt_pipe.src.test.external.util import make_random_nested_tens\n",
                "from mt_pipe.src.test.external.util import validate_nested_obj\n",
                "\n",
                "loss_fn = SSLFlowLoss()\n",
                "\n",
                "h, w = 64, 128\n",
                "bs = 8\n",
                "mock_batch = {\n",
                "    \"img1\": torch.Tensor(bs, 3, h, w),\n",
                "    \"img2\": torch.Tensor(bs, 3, h, w),\n",
                "}\n",
                "mock_info_conf = {\n",
                "    \"feature_pyramid_one\": {\n",
                "        \"emb\": [8, 768],\n",
                "        \"l6\": [8, 768, 1, 2],\n",
                "        \"l5\": [8, 384, 2, 4],\n",
                "        \"l4\": [8, 192, 4, 8],\n",
                "        \"l3\": [8, 96, 8, 16],\n",
                "        \"l2\": [8, 48, 16, 32],\n",
                "        \"l1\": [8, 48, 32, 64],\n",
                "    },\n",
                "    \"feature_pyramid_two\": {\n",
                "        \"emb\": [8, 768],\n",
                "        \"l6\": [8, 768, 1, 2],\n",
                "        \"l5\": [8, 384, 2, 4],\n",
                "        \"l4\": [8, 192, 4, 8],\n",
                "        \"l3\": [8, 96, 8, 16],\n",
                "        \"l2\": [8, 48, 16, 32],\n",
                "        \"l1\": [8, 48, 32, 64],\n",
                "    },\n",
                "    \"optical_flows\": [\n",
                "        [8, 2, 64, 128],\n",
                "        [8, 2, 32, 64],\n",
                "        [8, 2, 16, 32],\n",
                "        [8, 2, 8, 16],\n",
                "    ],\n",
                "    \"optical_flows_rev\": [\n",
                "        [8, 2, 64, 128],\n",
                "        [8, 2, 32, 64],\n",
                "        [8, 2, 16, 32],\n",
                "        [8, 2, 8, 16],\n",
                "    ],\n",
                "    \"img1_valid_masks\": [\n",
                "        [8, 1, 64, 128],\n",
                "        [8, 1, 32, 64],\n",
                "        [8, 1, 16, 32],\n",
                "        [8, 1, 8, 16],\n",
                "    ],\n",
                "    \"img2_valid_masks\": [\n",
                "        [8, 1, 64, 128],\n",
                "        [8, 1, 32, 64],\n",
                "        [8, 1, 16, 32],\n",
                "        [8, 1, 8, 16],\n",
                "    ],\n",
                "    \"fwd_flow_diff_pyramid\": [\n",
                "        [8, 2, 64, 128],\n",
                "        [8, 2, 32, 64],\n",
                "        [8, 2, 16, 32],\n",
                "        [8, 2, 8, 16],\n",
                "    ],\n",
                "    \"bwd_flow_diff_pyramid\": [\n",
                "        [8, 2, 64, 128],\n",
                "        [8, 2, 32, 64],\n",
                "        [8, 2, 16, 32],\n",
                "        [8, 2, 8, 16],\n",
                "    ],\n",
                "}\n",
                "mock_info = make_random_nested_tens(mock_info_conf)\n",
                "loss_pack = loss_fn(mock_info, mock_batch)\n",
                "expected_loss_pack_conf = {\n",
                "    \"tot\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "    \"loss_pixel\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "    \"loss_ssim\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "    \"loss_flow_smooth\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "    \"loss_flow_consis\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "}\n",
                "valid, msg = validate_nested_obj(loss_pack, expected_loss_pack_conf)\n",
                "assert valid, msg"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## GTFlowLoss\n",
                "\n",
                "TODO"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# DepthLoss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## SSL"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from mt_pipe.src.util.learner_mux import LearnerMux\n",
                "# from src.datasets import KITTIWithCalibration\n",
                "# from src.losses.depth_loss import DepthLoss\n",
                "# from torch.utils.data import DataLoader\n",
                "\n",
                "# loss_fn = DepthLoss(1)\n",
                "\n",
                "# ds = KITTIWithCalibration(\"./data/KITTI-2012\")\n",
                "# dl = DataLoader(ds, 4)\n",
                "\n",
                "# ln = LearnerMux(\n",
                "#     chldrn={\n",
                "#         \"flow_learner\": {\n",
                "#             \"target\": \"src.learners.flow.FlowLearner\",\n",
                "#             \"in_map\": {\"flow_path\": \"full\"},\n",
                "#             \"out_map\": {\"flow_path\": \"flow\"},\n",
                "#         },\n",
                "#         \"depth_learner\": {\n",
                "#             \"target\": \"src.learners.DepthLearner\",\n",
                "#             \"in_map\": {\n",
                "#                 \"depth_path_1\": {\"img1\": \"img\"},\n",
                "#                 \"depth_path_2\": {\"img2\": \"img\"},\n",
                "#             },\n",
                "#             \"out_map\": {\n",
                "#                 \"depth_path_1\": \"depth1\",\n",
                "#                 \"depth_path_2\": \"depth2\",\n",
                "#             },\n",
                "#         },\n",
                "#     },\n",
                "#     encoder={\n",
                "#         \"target\": \"src.models.backbone.BackBone\",\n",
                "#         \"params\": {\n",
                "#             \"enc_name\": \"ConvNeXt\",\n",
                "#         },\n",
                "#     },\n",
                "# )\n",
                "# ln.set_devices([0, 1])\n",
                "\n",
                "# for batch in dl:\n",
                "#     info = ln(batch)\n",
                "#     loss_pack = loss_fn(info, batch)\n",
                "#     print_deep_loss_pack(loss_pack)\n",
                "#     break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import torch\n",
                "# from src.losses.depth_loss import DepthLoss\n",
                "# from mt_pipe.src.test.external.util import make_random_nested_tens\n",
                "# from mt_pipe.src.test.external.util import validate_nested_obj\n",
                "\n",
                "# loss_fn = DepthLoss(0)\n",
                "\n",
                "# mock_batch = {\n",
                "#     \"img1\": torch.Tensor(4, 3, 64, 128),\n",
                "#     \"img2\": torch.Tensor(4, 3, 64, 128),\n",
                "#     \"K\": torch.Tensor(4, 1, 3, 3),\n",
                "#     \"K_inv\": torch.Tensor(4, 1, 3, 3),\n",
                "# }\n",
                "# mock_info_conf = {\n",
                "#     \"flow\": {\n",
                "#         \"feature_pyramid_one\": {\n",
                "#             \"emb\": (4, 768),\n",
                "#             \"l1\": (4, 768, 1, 2),\n",
                "#             \"l2\": (4, 384, 2, 4),\n",
                "#             \"l3\": (4, 192, 4, 8),\n",
                "#             \"l4\": (4, 96, 8, 16),\n",
                "#             \"l5\": (4, 48, 16, 32),\n",
                "#             \"l6\": (4, 48, 32, 64),\n",
                "#         },\n",
                "#         \"feature_pyramid_two\": {\n",
                "#             \"emb\": (4, 768),\n",
                "#             \"l1\": (4, 768, 1, 2),\n",
                "#             \"l2\": (4, 384, 2, 4),\n",
                "#             \"l3\": (4, 192, 4, 8),\n",
                "#             \"l4\": (4, 96, 8, 16),\n",
                "#             \"l5\": (4, 48, 16, 32),\n",
                "#             \"l6\": (4, 48, 32, 64),\n",
                "#         },\n",
                "#         \"warp_feature_pyramid_fwd\": {\n",
                "#             \"l2\": (4, 384, 2, 4),\n",
                "#             \"l3\": (4, 192, 4, 8),\n",
                "#             \"l4\": (4, 96, 8, 16),\n",
                "#             \"l5\": (4, 48, 16, 32),\n",
                "#             \"l6\": (4, 48, 32, 64),\n",
                "#             \"l7\": (4, 3, 64, 128),\n",
                "#         },\n",
                "#         \"warp_feature_pyramid_bwd\": {\n",
                "#             \"l2\": (4, 384, 2, 4),\n",
                "#             \"l3\": (4, 192, 4, 8),\n",
                "#             \"l4\": (4, 96, 8, 16),\n",
                "#             \"l5\": (4, 48, 16, 32),\n",
                "#             \"l6\": (4, 48, 32, 64),\n",
                "#             \"l7\": None,\n",
                "#         },\n",
                "#         \"flow_fwd\": {\n",
                "#             \"l2\": (4, 2, 2, 4),\n",
                "#             \"l3\": (4, 2, 4, 8),\n",
                "#             \"l4\": (4, 2, 8, 16),\n",
                "#             \"l5\": (4, 2, 16, 32),\n",
                "#             \"l6\": (4, 2, 32, 64),\n",
                "#             \"l7\": (4, 2, 64, 128),\n",
                "#         },\n",
                "#         \"flow_bwd\": {\n",
                "#             \"l2\": (4, 2, 2, 4),\n",
                "#             \"l3\": (4, 2, 4, 8),\n",
                "#             \"l4\": (4, 2, 8, 16),\n",
                "#             \"l5\": (4, 2, 16, 32),\n",
                "#             \"l6\": (4, 2, 32, 64),\n",
                "#             \"l7\": (4, 2, 64, 128),\n",
                "#         },\n",
                "#         \"img1_valid_mask\": {\n",
                "#             \"l2\": (4, 1, 2, 4),\n",
                "#             \"l3\": (4, 1, 4, 8),\n",
                "#             \"l4\": (4, 1, 8, 16),\n",
                "#             \"l5\": (4, 1, 16, 32),\n",
                "#             \"l6\": (4, 1, 32, 64),\n",
                "#             \"l7\": (4, 1, 64, 128),\n",
                "#         },\n",
                "#         \"img2_valid_mask\": {\n",
                "#             \"l2\": (4, 1, 2, 4),\n",
                "#             \"l3\": (4, 1, 4, 8),\n",
                "#             \"l4\": (4, 1, 8, 16),\n",
                "#             \"l5\": (4, 1, 16, 32),\n",
                "#             \"l6\": (4, 1, 32, 64),\n",
                "#             \"l7\": (4, 1, 64, 128),\n",
                "#         },\n",
                "#         \"img1_flow_diff_mask\": {\n",
                "#             \"l2\": (4, 2, 2, 4),\n",
                "#             \"l3\": (4, 2, 4, 8),\n",
                "#             \"l4\": (4, 2, 8, 16),\n",
                "#             \"l5\": (4, 2, 16, 32),\n",
                "#             \"l6\": (4, 2, 32, 64),\n",
                "#             \"l7\": (4, 2, 64, 128),\n",
                "#         },\n",
                "#         \"img2_flow_diff_mask\": {\n",
                "#             \"l2\": (4, 2, 2, 4),\n",
                "#             \"l3\": (4, 2, 4, 8),\n",
                "#             \"l4\": (4, 2, 8, 16),\n",
                "#             \"l5\": (4, 2, 16, 32),\n",
                "#             \"l6\": (4, 2, 32, 64),\n",
                "#             \"l7\": (4, 2, 64, 128),\n",
                "#         },\n",
                "#     },\n",
                "#     \"depth1\": {\n",
                "#         \"l2\": (4, 1, 2, 4),\n",
                "#         \"l3\": (4, 1, 4, 8),\n",
                "#         \"l4\": (4, 1, 8, 16),\n",
                "#         \"l5\": (4, 1, 16, 32),\n",
                "#         \"l6\": (4, 1, 32, 64),\n",
                "#         \"l7\": (4, 1, 64, 128),\n",
                "#         \"pred\": (4, 1, 64, 128),\n",
                "#     },\n",
                "#     \"depth2\": {\n",
                "#         \"l2\": (4, 1, 2, 4),\n",
                "#         \"l3\": (4, 1, 4, 8),\n",
                "#         \"l4\": (4, 1, 8, 16),\n",
                "#         \"l5\": (4, 1, 16, 32),\n",
                "#         \"l6\": (4, 1, 32, 64),\n",
                "#         \"l7\": (4, 1, 64, 128),\n",
                "#         \"pred\": (4, 1, 64, 128),\n",
                "#     },\n",
                "# }\n",
                "# mock_info = make_random_nested_tens(mock_info_conf)\n",
                "# loss_pack = loss_fn(mock_info, mock_batch)\n",
                "\n",
                "\n",
                "# expected_loss_pack_conf = {\n",
                "#     \"tot\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "#     \"pt_depth_loss\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "#     \"pj_depth_loss\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "#     \"flow_loss\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "#     \"depth_smooth_loss\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "# }\n",
                "# tentative_none_mask = {\n",
                "#     \"pt_depth_loss\": None,\n",
                "#     \"pj_depth_loss\": None,\n",
                "#     \"flow_loss\": None,\n",
                "#     \"depth_smooth_loss\": None,\n",
                "# }\n",
                "# valid, msg = validate_nested_obj(loss_pack, expected_loss_pack_conf, tentative_none_mask)\n",
                "# assert valid, msg"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Ground Truth"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from src.learners import DepthLearner\n",
                "# from src.datasets import KITTIWithDepth\n",
                "# from src.losses import GTDepthLoss\n",
                "# from torch.utils.data import DataLoader\n",
                "\n",
                "# ds = KITTIWithDepth(\"./data/KITTI-2015/\")\n",
                "# dl = DataLoader(ds, 8)\n",
                "# batch = next(iter(dl))\n",
                "\n",
                "# loss_fn = GTDepthLoss(device=0)\n",
                "# encoder_conf = {\"target\": \"src.models.BackBone\", \"params\": {\"enc_name\": \"ConvNeXt\"}}\n",
                "# ln = DepthLearner(encoder_conf)\n",
                "# ln.set_devices([0, 0])\n",
                "# info = ln(batch)\n",
                "# loss_pack = loss_fn(info, batch)\n",
                "\n",
                "# print_deep_loss_pack(loss_pack)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# from src.losses import GTDepthLoss\n",
                "# from mt_pipe.src.test.external.util import validate_nested_obj, make_random_nested_tens\n",
                "\n",
                "# loss_fn = GTDepthLoss()\n",
                "# mock_info_conf = {\n",
                "#     \"l2\": [8, 1, 2, 4],\n",
                "#     \"l3\": [8, 1, 4, 8],\n",
                "#     \"l4\": [8, 1, 8, 16],\n",
                "#     \"l5\": [8, 1, 16, 32],\n",
                "#     \"l6\": [8, 1, 32, 64],\n",
                "#     \"l7\": [8, 1, 64, 128],\n",
                "#     \"pred\": [8, 1, 64, 128],\n",
                "# }\n",
                "# mock_info = make_random_nested_tens(mock_info_conf)\n",
                "# mock_batch_conf = {\"img\": [8, 3, 64, 128], \"depth_map\": [8, 1, 64, 128]}\n",
                "# mock_batch = make_random_nested_tens(mock_batch_conf)\n",
                "# loss_pack = loss_fn(mock_info, mock_batch)\n",
                "# expected_loss_pack_conf = {\n",
                "#     \"tot\": {\"shape\": [], \"dtype\": \"torch.float32\"},\n",
                "#     \"L1\": {\"shape\": [], \"dtype\": \"torch.float32\"},\n",
                "#     \"Smooth\": {\"shape\": [], \"dtype\": \"torch.float32\"},\n",
                "# }\n",
                "# valid, msg = validate_nested_obj(loss_pack, expected_loss_pack_conf)\n",
                "# assert valid, msg"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# All Losses"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from mt_pipe.src.util.learner_mux import LearnerMux\n",
                "from src.datasets import KITTIWithCalibration, ImageNetVICReg\n",
                "from torch.utils.data import DataLoader\n",
                "from mt_pipe.src.losses import ConcatLoss\n",
                "\n",
                "loss_fn = ConcatLoss(\n",
                "    {\n",
                "        \"content\": 0,\n",
                "        \"flow\": 0,\n",
                "        # \"depth\":0,\n",
                "    },\n",
                "    conf={\n",
                "        \"content\": {\n",
                "            \"target\": \"src.losses.ContentLoss\",\n",
                "            \"branch\": \"content\",\n",
                "            \"params\": {\n",
                "                \"loss_weights\": {\"vc_loss_X\": [0.01, 0.04], \"vic_loss_Y\": [25, 1, 1]}\n",
                "            },\n",
                "        },\n",
                "        \"flow\": {\n",
                "            \"target\": \"src.losses.SSLFlowLoss\",\n",
                "            \"branch\": \"flow\",\n",
                "            # \"params\": {\n",
                "            #     \"loss_weights\": {\n",
                "            #         \"cycle_loss\": 0.2,\n",
                "            #         \"reconstruction_loss\": 1,\n",
                "            #         \"reconstruction_loss_coeffs\": [1, 1, 1],\n",
                "            #         \"regression_loss\": 1,\n",
                "            #         \"smooth_loss\": 75,\n",
                "            #         \"vc_loss\": 1,\n",
                "            #         \"vc_loss_coeffs\": {\n",
                "            #             \"l1\": [0.01, 0.04],\n",
                "            #             \"l2\": [0.01, 0.04],\n",
                "            #             \"l3\": [0.01, 0.001],\n",
                "            #             \"l4\": [0.01, 0],\n",
                "            #             \"l5\": [0.001, 0],\n",
                "            #             \"l6\": [0.0001, 0],\n",
                "            #         },\n",
                "            #     }\n",
                "            # },\n",
                "        },\n",
                "        # \"depth\": {\"target\": \"src.losses.DepthLoss\"},\n",
                "    },\n",
                ")\n",
                "\n",
                "ds1 = KITTIWithCalibration(\"./data/KITTI-2012\", img_wh=[128, 64])\n",
                "dl1 = DataLoader(ds1, 4)\n",
                "ds2 = ImageNetVICReg(\"./data/ImageNet-2012\", img_wh=[64, 64])\n",
                "dl2 = DataLoader(ds2, 4)\n",
                "\n",
                "ln = LearnerMux(\n",
                "    chldrn={\n",
                "        \"flow_learner\": {\n",
                "            \"target\": \"src.learners.FlowLearner\",\n",
                "            \"in_map\": {\"flow_path\": \"full\"},\n",
                "            \"out_map\": {\"flow_path\": \"flow\"},\n",
                "        },\n",
                "        # \"depth_learner\": {\n",
                "        #     \"target\": \"src.learners.DepthLearner\",\n",
                "        #     \"in_map\": {\n",
                "        #         \"depth_path_1\": {\"img1\": \"img\"},\n",
                "        #         \"depth_path_2\": {\"img2\": \"img\"},\n",
                "        #     },\n",
                "        #     \"out_map\": {\n",
                "        #         \"depth_path_1\": \"depth1\",\n",
                "        #         \"depth_path_2\": \"depth2\",\n",
                "        #     },\n",
                "        # },\n",
                "        \"content_learner\": {\n",
                "            \"target\": \"src.learners.ContentLearner\",\n",
                "            \"in_map\": {\"content_path\": \"full\"},\n",
                "            \"out_map\": {\"content_path\": \"content\"},\n",
                "        },\n",
                "    },\n",
                "    encoder={\n",
                "        \"target\": \"src.models.backbone.BackBone\",\n",
                "        \"params\": {\n",
                "            \"enc_name\": \"ConvNeXt\",\n",
                "        },\n",
                "    },\n",
                ")\n",
                "ln.set_devices([0, 1])\n",
                "\n",
                "for batch1, batch2 in zip(dl1, dl2):\n",
                "    batch = {**batch1, **batch2}\n",
                "    info = ln(batch)\n",
                "    loss_pack = loss_fn(info, batch)\n",
                "    print_deep_loss_pack(loss_pack)\n",
                "    break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Segmentation Loss"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.learners import SegmentLearner\n",
                "from src.models import BackBone\n",
                "from src.datasets import COCOSegment\n",
                "from src.losses import SegmentationLoss\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "ds = COCOSegment(\"./data/COCO-2017\")\n",
                "dl = DataLoader(ds, 8)\n",
                "\n",
                "bb = BackBone(\"ConvNeXt\").cuda(0)\n",
                "ln = SegmentLearner(bb, n_classes=80)\n",
                "ln.set_devices([0,1])\n",
                "loss_fn = SegmentationLoss()\n",
                "\n",
                "for batch in dl:\n",
                "    info = ln(batch)\n",
                "    loss_pack = loss_fn(info, batch)\n",
                "    print_deep_loss_pack(loss_pack)\n",
                "    break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.losses import SegmentationLoss\n",
                "import torch\n",
                "from mt_pipe.src.test.external.util import validate_nested_obj\n",
                "\n",
                "loss_fn = SegmentationLoss()\n",
                "mock_info = {\n",
                "    \"seg\": torch.Tensor(80, 128, 128)\n",
                "}\n",
                "mock_batch = {\n",
                "    \"seg\": torch.Tensor(80, 128, 128)\n",
                "}\n",
                "loss_pack = loss_fn(mock_info, mock_batch)\n",
                "expected_loss_pack_conf = {\n",
                "    \"tot\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "    \"Dice\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "    \"BCEWithLogits\": {\"shape\": (), \"dtype\": \"torch.float32\"},\n",
                "}\n",
                "valid, msg = validate_nested_obj(loss_pack, expected_loss_pack_conf)\n",
                "assert valid, msg"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
