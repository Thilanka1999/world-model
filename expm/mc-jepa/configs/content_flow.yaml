name: content_flow
data:
    content:
        target: src.datasets.ImageNetVICReg
        params:
            root: ImageNet-2012/
            img_wh: [224, 224]
    flow:
        target: mt_pipe.src.datasets.ConcatSet
        params:
            root:
                - KITTI-2012
                - KITTI
            conf:
                - target: src.datasets.kitti.KITTI
                  reps: 100
                  split_mix:
                      train: [train, val]
                  params:
                      img_wh: [512, 256]
                - target: src.datasets.kitti.KITTI
                  reps: 1
                  params:
                      img_wh: [512, 256]
learner:
    target: mt_pipe.src.util.learner_mux.LearnerMux
    local_device_maps:
        flow_learner: [0, 1]
        content_learner: [0, 0]
    params:
        chldrn:
            flow_learner:
                target: src.learners.FlowLearner
                params:
                    img_wh: [512, 256] # TODO: this doesn't work (LearnerMux)
                # in_map: # can be removed or passed since this is the default
                #     flow_path: full
                out_map:
                    flow_path: flow
            content_learner:
                target: src.learners.ContentLearner
                # in_map:  # can be removed or passed since this is the default
                #     content_path: full
                out_map:
                    content_path: content
        encoder:
            target: src.models.backbone.BackBone
            params:
                enc_name: ConvNeXt
loss:
    target: mt_pipe.src.losses.ConcatLoss
    local_device_maps:
        content: 0
        flow: 0
    params:
        conf:
            content:
                target: src.losses.ContentLoss
                branch: content
                params:
                    loss_weights:
                        vc_loss_X: [0.01, 0.04]
                        vc_loss_Y: [25, 1]
            flow:
                target: src.losses.FlowLoss
                branch: flow
                params:
                    loss_weights:
                        cycle_loss: 0.2
                        reconstruction_loss: 1
                        reconstruction_loss_coeffs: [1, 1, 1]
                        regression_loss: 1
                        smooth_loss: 75
                        vc_loss: 1
                        vc_loss_coeffs:
                            l1: [0.01, 0.04]
                            l2: [0.01, 0.04]
                            l3: [0.01, 0.001]
                            l4: [0.01, 0]
                            l5: [0.001, 0]
                            l6: [0.0001, 0]
optimizer:
    target: torch.optim.AdamW
    params:
        lr: 3e-4
        betas: [0.9, 0.999]
        weight_decay: 1e-6
lr_scheduler:
    target: torch.optim.lr_scheduler.CosineAnnealingLR
    params:
        T_max: 20
        eta_min: 3e-8
train:
    loader_params:
        content:
            batch_size: 4 # 384
            num_workers: 1
        flow:
            batch_size: 2 # 2
            num_workers: 1
    tollerance: 5
    epochs: 100
